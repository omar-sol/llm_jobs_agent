{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def flatten_dict(d, parent_key='', sep='_', parent_keys=set()):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        # Check if key is unique in the context of parent keys\n",
    "        new_key = k if (k not in parent_keys) else f'{parent_key}{sep}{k}' if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            # Update the set of parent keys for the next level\n",
    "            new_parent_keys = parent_keys.union(set(d.keys()))\n",
    "            items.extend(flatten_dict(v, new_key, sep, new_parent_keys).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def read_jsonl_and_flatten(file_path):\n",
    "    flattened_data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Load the line as a JSON object\n",
    "            data_dict = json.loads(line)\n",
    "            # Flatten the dictionary\n",
    "            flattened_dict = flatten_dict(data_dict)\n",
    "            \n",
    "            # Might be better to clean this in another function\n",
    "            # Check if the dictionary has the right number of keys\n",
    "            if len(flattened_dict.keys()) != 34:\n",
    "                print(\"Skipping due to wrong number of keys\")\n",
    "                print(flattened_dict.keys())\n",
    "                print(flattened_dict)\n",
    "                continue \n",
    "\n",
    "            if flattened_dict[\"jobs_towardsai_url\"] == 'nan':\n",
    "                print(\"Skipping due to nan url\")\n",
    "                print(flattened_dict)\n",
    "                continue\n",
    "            \n",
    "            # Add the flattened dictionary to the list\n",
    "            flattened_data.append(flattened_dict)\n",
    "\n",
    "    return flattened_data\n",
    "\n",
    "# Example usage\n",
    "file_path = '../data/formatted_jobs_feb5_24_results.jsonl'\n",
    "flattened_data = read_jsonl_and_flatten(file_path)\n",
    "print(\"end of file\")\n",
    "print(flattened_data[0])\n",
    "print(len(flattened_data[0].keys()))\n",
    "print(flattened_data[9].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def optimize_keys(data_dict):\n",
    "    # Function to check if a value is valid (not None, Nan, empty string, or 'Not Specified')\n",
    "\n",
    "    def is_valid(value):\n",
    "        # Check for None\n",
    "        if value is None:\n",
    "            return False\n",
    "\n",
    "        # Check for NaN for float values\n",
    "        if isinstance(value, float) and math.isnan(value):\n",
    "            return False\n",
    "\n",
    "        # Check for empty strings and lists\n",
    "        if value == \"\" or value == []:\n",
    "            return False\n",
    "\n",
    "        # Case-insensitive check for specific strings\n",
    "        if isinstance(value, str) and value.lower() in [\"nan\", \"not specified\"]:\n",
    "            return False\n",
    "\n",
    "        # If value is a list, check each item\n",
    "        if isinstance(value, list):\n",
    "            for item in value:\n",
    "                # Check if the item is a string and matches the specified values\n",
    "                if isinstance(item, str) and item.lower() in [\"nan\", \"not specified\"]:\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    # Optimize 'city' and 'country'\n",
    "    # If location_city is valid use it ('country'='JP\" and location_country='Japan')\n",
    "    for key in [\"city\", \"country\"]:\n",
    "        location_model_key = f\"location_model_{key}\"\n",
    "        # if not is_valid(data_dict.get(key)) and is_valid(\n",
    "        #     data_dict.get(location_model_key)\n",
    "        # ):\n",
    "        if is_valid(data_dict.get(location_model_key)):\n",
    "            data_dict[key] = data_dict[location_model_key]\n",
    "        data_dict.pop(location_model_key, None)\n",
    "\n",
    "    # Optimize 'ai' and 'involves_ai'\n",
    "    if data_dict.get(\"ai\") not in [0, 1] and is_valid(data_dict.get(\"involves_ai\")):\n",
    "        data_dict[\"ai\"] = 1 if data_dict[\"involves_ai\"] else 0\n",
    "    data_dict.pop(\"involves_ai\", None)\n",
    "    if data_dict.get(\"ai\") in [0, 1]:\n",
    "        data_dict[\"ai\"] = bool(data_dict[\"ai\"])\n",
    "\n",
    "    # Optimize 'salary_numerical'\n",
    "    salary = data_dict.get(\"salary_numerical\")\n",
    "    print(\"salary\", salary)\n",
    "    if not is_valid(salary) or isinstance(salary, str):\n",
    "        data_dict[\"salary_min\"] = data_dict[\"salary_max\"] = None\n",
    "    elif (\n",
    "        isinstance(salary, list)\n",
    "        and len(salary) == 1\n",
    "        and is_valid(salary[0])\n",
    "        and salary[0] < 900000\n",
    "        and salary[0] > 5\n",
    "    ):\n",
    "        data_dict[\"salary_min\"] = data_dict[\"salary_max\"] = salary[0]\n",
    "    elif (\n",
    "        isinstance(salary, list)\n",
    "        and len(salary) == 2\n",
    "        and all(is_valid(val) for val in salary)\n",
    "        and all(val < 900000 for val in salary)\n",
    "        and all(val > 5 for val in salary)\n",
    "    ):\n",
    "        data_dict[\"salary_min\"], data_dict[\"salary_max\"] = salary\n",
    "    else:\n",
    "        data_dict[\"salary_min\"] = data_dict[\"salary_max\"] = None\n",
    "\n",
    "    print(\"salary_min\", data_dict[\"salary_min\"])\n",
    "    print(\"salary_max\", data_dict[\"salary_max\"])\n",
    "    print(\"salary_frequency\", data_dict[\"salary_frequency\"], \"\\n\")\n",
    "    data_dict.pop(\"salary_numerical\", None)\n",
    "\n",
    "    # Optimize 'job_skills'\n",
    "    if is_valid(data_dict.get(\"required_skills\")) and not is_valid(data_dict.get(\"skills\")):\n",
    "        data_dict[\"skills\"] = data_dict[\"required_skills\"]\n",
    "    elif is_valid(data_dict.get(\"skills\")):\n",
    "        data_dict[\"skills\"] = data_dict.get(\"skills\").split(\",\")\n",
    "    data_dict.pop(\"required_skills\", None)\n",
    "\n",
    "    # Optimize 'experience_years'\n",
    "    if is_valid(data_dict.get(\"experience_years\")):\n",
    "        experience_years = data_dict[\"experience_years\"]\n",
    "        if isinstance(experience_years, str):\n",
    "            data_dict[\"experience_years\"] = None\n",
    "        else:\n",
    "            data_dict[\"experience_years\"] = int(experience_years)\n",
    "\n",
    "    # Replace 'Not specified' with None for all keys\n",
    "    for key, value in data_dict.items():\n",
    "        if not is_valid(value):\n",
    "            data_dict[key] = None\n",
    "\n",
    "    # Transform lists into strings\n",
    "    for key, value in data_dict.items():\n",
    "        if isinstance(value, list):\n",
    "            data_dict[key] = \", \".join(value) if len(value) > 1 else value[0]\n",
    "\n",
    "    # Transform values into lowercase strings\n",
    "    lowercased_data = {\n",
    "        key: value.lower() if isinstance(value, str) else value\n",
    "        for key, value in data_dict.items()\n",
    "    }\n",
    "\n",
    "    return lowercased_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts = []\n",
    "for i, dict in enumerate(flattened_data):\n",
    "    print(\"index\", i)\n",
    "    optimized_dict = optimize_keys(dict)\n",
    "    list_of_dicts.append(optimized_dict)\n",
    "\n",
    "# print(list_of_dicts[8978].keys())\n",
    "print(list_of_dicts[8978])\n",
    "# for i, dict in enumerate(list_of_dicts):\n",
    "# print(list_of_dicts[2])\n",
    "# print(dict.keys())\n",
    "# if i == 5:\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# The data in a dataframe\n",
    "df1 = pd.DataFrame(list_of_dicts)\n",
    "df1 = df1.rename(columns={\"ai\": \"involves_ai\"})\n",
    "df1 = df1.rename(columns={\"title\": \"job_title\"})\n",
    "df1 = df1.rename(columns={\"skills\": \"job_skills\"})\n",
    "\n",
    "df1['created_at'] = pd.to_datetime(df1['created_at'])\n",
    "df1 = df1.rename(columns={\"created_at\": \"creation_date\"})\n",
    "\n",
    "# df1 = df1.rename(columns={\"cleaned_description\": \"job_listing_text\"})\n",
    "df1 = df1.drop(\n",
    "    [\"slug\", \"approved\", \"chain_of_thought\", \"job_type_reasoning\", \"remote_reasoning\"],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        num_tokens = len(encoding.encode(string))\n",
    "        return num_tokens\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating number of tokens: {e}\")\n",
    "        return 0\n",
    "\n",
    "df1[\"num_tokens\"] = df1[\"job_listing_text\"].apply(\n",
    "        lambda x: num_tokens_from_string(x, \"cl100k_base\")\n",
    "    )\n",
    "\n",
    "# # Finding and printing the max value in the salary_max column\n",
    "# max_salary_max = df1['salary_max'].max()\n",
    "# print(f\"Maximum value in 'salary_max' column: {max_salary_max}\")\n",
    "\n",
    "# # Finding and printing the min value in the salary_min column\n",
    "# min_salary_min = df1['salary_min'].min()\n",
    "# print(f\"Minimum value in 'salary_min' column: {min_salary_min}\")\n",
    "\n",
    "# # Print the row with the max salary_max value\n",
    "# print(\"Row with the maximum 'salary_max' value:\")\n",
    "# print(df1[df1['salary_max'] == max_salary_max])\n",
    "\n",
    "# # Print the row with the min salary_min value\n",
    "# print(\"\\nRow with the minimum 'salary_min' value:\")\n",
    "# print(df1[df1['salary_min'] == min_salary_min])\n",
    "\n",
    "# print(df1.head(1))\n",
    "# print(df1.dtypes)\n",
    "# print(\"\\n\")\n",
    "print(df1.info())\n",
    "# print(df1.shape)\n",
    "# print(df1.columns)\n",
    "# print(len(df1))\n",
    "\n",
    "# df1.to_pickle(\"../data/extracted_cleaned_df_feb5.pkl\")\n",
    "# print(df1.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8165 entries, 0 to 8164\n",
      "Data columns (total 20 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   job_id                   8165 non-null   int64         \n",
      " 1   created_at               8165 non-null   datetime64[ns]\n",
      " 2   job_title                8165 non-null   object        \n",
      " 3   job_skills               8165 non-null   object        \n",
      " 4   job_type                 8165 non-null   object        \n",
      " 5   company_id               8165 non-null   int64         \n",
      " 6   apply_url                8165 non-null   object        \n",
      " 7   city                     8165 non-null   object        \n",
      " 8   country                  8165 non-null   object        \n",
      " 9   salary                   8165 non-null   object        \n",
      " 10  salary_min               8165 non-null   int64         \n",
      " 11  salary_max               8165 non-null   int64         \n",
      " 12  salary_currency          8165 non-null   object        \n",
      " 13  url_slug                 8165 non-null   object        \n",
      " 14  role_description         8165 non-null   object        \n",
      " 15  company_name             8165 non-null   object        \n",
      " 16  company_description      8165 non-null   object        \n",
      " 17  cleaned_description      8165 non-null   object        \n",
      " 18  scraped_skills_required  8165 non-null   object        \n",
      " 19  scraped_skills_useful    8165 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(4), object(15)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"db_info.json\")\n",
    "print(df.info())\n",
    "# print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and upload dataset to Deep Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import time\n",
    "# import os\n",
    "# from buster.documents_manager import DeepLakeDocumentsManager\n",
    "# from dotenv import load_dotenv\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import deeplake\n",
    "# import tiktoken\n",
    "\n",
    "# load_dotenv(\".env\")\n",
    "# ACTIVELOOP_TOKEN = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "\n",
    "\n",
    "# # df1 = pd.read_csv(\"../data/jobs_original_data_2.csv\", low_memory=False, on_bad_lines='warn')\n",
    "# df1 = pd.read_json(\"../data/formatted_jobs.json\")\n",
    "# df1['created_at'] = df1['created_at'].astype(str) # convert to string, else its not json serializable\n",
    "# print(len(df1))\n",
    "# # print(df1.dtypes)\n",
    "# # print(\"\\n\")\n",
    "# # print(df1.info())\n",
    "# # print(df1.shape)\n",
    "# # print(df1.columns)\n",
    "\n",
    "# df1 = df1.dropna(subset=\"cleaned_description\")\n",
    "# # df1.to_csv(\"../data/jobs_cleaned_data2.csv\", index=False)\n",
    "\n",
    "\n",
    "# df1 = df1.rename(columns={'cleaned_description': 'content'})\n",
    "\n",
    "\n",
    "# def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "#     \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "#     encoding = tiktoken.get_encoding(encoding_name)\n",
    "#     num_tokens = len(encoding.encode(string))\n",
    "#     return num_tokens\n",
    "\n",
    "# df1['num_tokens'] = df1['content'].apply(lambda x: num_tokens_from_string(x, \"cl100k_base\"))\n",
    "\n",
    "# # Now, you can filter the DataFrame to only keep rows with num_tokens less than 8000\n",
    "# df1_filtered = df1[df1['num_tokens'] < 8000]\n",
    "# print(\"number of rows:\", len(df1_filtered))\n",
    "# df1_filtered.drop('num_tokens', axis=1, inplace=True)\n",
    "\n",
    "# # df2 = df1.head(5000)\n",
    "\n",
    "\n",
    "# # # # dataset_path = \"hub://towards_ai/ai-jobs-dataset\"\n",
    "# dataset_path = \"local_dataset\"\n",
    "\n",
    "# dm = DeepLakeDocumentsManager(\n",
    "#     vector_store_path=dataset_path,\n",
    "#     overwrite=True, # does not work when the dataset is empty?\n",
    "#     # required_columns=[\"url\", \"content\", \"source\", \"title\"],\n",
    "# )\n",
    "\n",
    "# # # Check if embeddings are present, computes them if not\n",
    "# # if \"embedding\" not in df.columns:\n",
    "# #     # df[\"embedding\"] = compute_embeddings_parallelized(df, embedding_fn=embedding_fn, num_workers=num_workers)\n",
    "# #     df.loc[:, \"embedding\"] = compute_embeddings_parallelized(df, embedding_fn=embedding_fn, num_workers=num_workers)\n",
    "\n",
    "# dm.batch_add(\n",
    "#     df=df1_filtered,\n",
    "#     batch_size=10000, # very important to this be greater than the whole dataset\n",
    "#     min_time_interval=60,\n",
    "#     num_workers=32,\n",
    "#     csv_overwrite=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import deeplake\n",
    "# # Renaming source 'langchain to 'langchain_docs'\n",
    "# # db = deeplake.VectorStore(\"local_dataset\", overwrite=False, read_only=False)\n",
    "# # db = deeplake.VectorStore(\"hub://towards_ai/ai-jobs\", overwrite=False, read_only=True)\n",
    "# # db.summary()\n",
    "# # metadata_list = db.dataset[\"metadata\"].data(aslist=True)['value']\n",
    "# # print(len(db.dataset[\"metadata\"]))\n",
    "# # print(len(metadata_list))\n",
    "# deeplake.deepcopy(\"local_dataset\", \"hub://towards_ai/ai-jobs-data-9105-3072\", overwrite=True, read_only=False)\n",
    "# # db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "# from dotenv import load_dotenv\n",
    "# import nested_structure as bm\n",
    "\n",
    "# load_dotenv(\".env\")\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# from IPython.display import Markdown, display\n",
    "# from llama_index import SQLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import (\n",
    "#     create_engine,\n",
    "#     MetaData,\n",
    "#     Table,\n",
    "#     Column,\n",
    "#     String,\n",
    "#     Integer,\n",
    "#     Float,\n",
    "#     Boolean,\n",
    "#     Text,\n",
    "# )\n",
    "# from sqlalchemy import insert\n",
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the engine and metadata\n",
    "# engine = create_engine(\"sqlite:///:memory:\")\n",
    "# metadata_obj = MetaData()\n",
    "\n",
    "# # Modify the table definition to include salary_min and salary_max\n",
    "# job_listing_details_table = Table(\n",
    "#     \"job_listing_details\",\n",
    "#     metadata_obj,\n",
    "#     Column(\"jobs_towardsai_url\", String, primary_key=True),\n",
    "\n",
    "# )\n",
    "\n",
    "# # Recreate the table with the new structure\n",
    "# metadata_obj.create_all(engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create city SQL table\n",
    "# table_name = \"city_stats\"\n",
    "# city_stats_table = Table(\n",
    "#     table_name,\n",
    "#     metadata_obj,\n",
    "#     Column(\"city_name\", String(16), primary_key=True),\n",
    "#     Column(\"population\", Integer),\n",
    "#     Column(\"country\", String(16), nullable=False),\n",
    "# )\n",
    "# metadata_obj.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import SQLDatabase, ServiceContext\n",
    "# from llama_index.llms import OpenAI\n",
    "\n",
    "# llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\n",
    "# service_context = ServiceContext.from_defaults(llm=llm)\n",
    "# # sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
    "# sql_database = SQLDatabase(engine, include_tables=[\"job_listing_details\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
    "# from sqlalchemy import insert\n",
    "\n",
    "# rows = [\n",
    "#     {\"city_name\": \"Toronto\", \"population\": 2930000, \"country\": \"Canada\"},\n",
    "#     {\"city_name\": \"Tokyo\", \"population\": 13960000, \"country\": \"Japan\"},\n",
    "#     {\n",
    "#         \"city_name\": \"Chicago\",\n",
    "#         \"population\": 2679000,\n",
    "#         \"country\": \"United States\",\n",
    "#     },\n",
    "#     {\"city_name\": \"Seoul\", \"population\": 9776000, \"country\": \"South Korea\"},\n",
    "# ]\n",
    "# for row in rows:\n",
    "#     stmt = insert(job_listing_details_table).values(**row)\n",
    "#     with engine.begin() as connection:\n",
    "#         cursor = connection.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # view current table\n",
    "# stmt = select(\n",
    "#     city_stats_table.c.city_name,\n",
    "#     city_stats_table.c.population,\n",
    "#     city_stats_table.c.country,\n",
    "# ).select_from(city_stats_table)\n",
    "\n",
    "# with engine.connect() as connection:\n",
    "#     results = connection.execute(stmt).fetchall()\n",
    "#     print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import text\n",
    "\n",
    "# with engine.connect() as con:\n",
    "#     rows = con.execute(text(\"SELECT * FROM job_listing_details ORDER BY CAST(salary_numerical AS REAL) DESC;\"))\n",
    "#     i = 0\n",
    "#     for row in rows:\n",
    "#         print(row)\n",
    "#         i += 1\n",
    "#         if i > 10:\n",
    "#             break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.indices.struct_store.sql_query import NLSQLTableQueryEngine\n",
    "\n",
    "# query_engine = NLSQLTableQueryEngine(\n",
    "#     sql_database=sql_database,\n",
    "#     tables=[\"job_listing_details\"],\n",
    "#     verbose=True,\n",
    "# )\n",
    "# query_str = \"Which job has the highest salary?\"\n",
    "# response = query_engine.query(query_str)\n",
    "# display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy.sql import select\n",
    "\n",
    "# # Function to fetch and display 5 elements from the database\n",
    "# def fetch_five_records(engine, table):\n",
    "#     # Create a SELECT query that selects all columns from the table\n",
    "#     query = select(table).limit(5)\n",
    "\n",
    "#     # Execute the query\n",
    "#     with engine.connect() as connection:\n",
    "#         result = connection.execute(query)\n",
    "\n",
    "#         # Fetch the results\n",
    "#         records = result.fetchall()\n",
    "\n",
    "#         # Display the records\n",
    "#         for record in records:\n",
    "#             print(record)\n",
    "\n",
    "# # Call the function to fetch and display records\n",
    "# fetch_five_records(engine, job_listing_details_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
